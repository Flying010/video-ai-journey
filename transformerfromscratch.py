# -*- coding: utf-8 -*-
"""TransformerFromScratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IHXh8WiQLNxYN-WuJzfO3sxAkXE6NXrY
"""

!pip install --upgrade setuptools
import os
os.environ["SETUPTOOLS_USE_DISTUTILS"] = "stdlib"

## First, check to see if lightning is installed, if not, install it.
import pip
try:
  __import__("lightning")
except ImportError:
  pip.main(['install', "lightning"])

import torch ## torch let's us create tensors and also provides helper functions
import torch.nn as nn ## torch.nn gives us nn.Module(), nn.Embedding() and nn.Linear()
import torch.nn.functional as F # This gives us the softmax() and argmax()
from torch.optim import Adam ## We will use the Adam optimizer, which is, essentially,
                             ## a slightly less stochastic version of stochastic gradient descent.
from torch.utils.data import TensorDataset, DataLoader ## We'll store our data in DataLoaders

import lightning as L ## Lightning makes it easier to write, optimize and scale our code

token_to_id = {'what': 0,
               'is': 1,
               'statquest': 2,
               'awesome': 3,
               '<EOS>': 4,
               }
id_to_token = dict(map(reversed, token_to_id.items()))

inputs = torch.tensor([[token_to_id["what"],
                        token_to_id["is"],
                        token_to_id["statquest"],
                        token_to_id["awesome"],
                        token_to_id["<EOS>"]],

                       [token_to_id["statquest"],
                        token_to_id["is"],
                        token_to_id["what"],
                        token_to_id["awesome"],
                        token_to_id["<EOS>"]]])

labels = torch.tensor([[token_to_id["is"],
                        token_to_id["statquest"],
                        token_to_id["<EOS>"],
                        token_to_id["awesome"],
                        token_to_id["<EOS>"]],

                       [token_to_id["is"],
                        token_to_id["what"],
                        token_to_id["<EOS>"],
                        token_to_id["awesome"],
                        token_to_id["<EOS>"]]])
dataset = TensorDataset(inputs, labels)
dataloader = DataLoader(dataset)

## first, we create a dictionary that maps vocabulary tokens to id numbers...
token_to_id = {'what' : 0,
               'is' : 1,
               'statquest' : 2,
               'awesome': 3,
               '<EOS>' : 4, ## <EOS> = end of sequence
              }
## ...then we create a dictionary that maps the ids to tokens. This will help us interpret the output.
## We use the "map()" function to apply the "reversed()" function to each tuple (i.e. ('what', 0)) stored
## in the token_to_id dictionary. We then use dict() to make a new dictionary from the
## reversed tuples.
id_to_token = dict(map(reversed, token_to_id.items()))

## NOTE: Because we are using a Decoder-Only Transformer, the inputs contain
##       the questions ("what is statquest?" and "statquest is what?") followed
##       by an <EOS> token followed by the response, "awesome".
##       This is because all of those tokens will be used as inputs to the Decoder-Only
##       Transformer during Training. (See the illustration above for more details)
## ALSO NOTE: When we train this way, it's called "teacher forcing".
##       Teacher forcing helps us train the neural network faster.
inputs = torch.tensor([[token_to_id["what"], ## input #1: what is statquest <EOS> awesome
                        token_to_id["is"],
                        token_to_id["statquest"],
                        token_to_id["<EOS>"],
                        token_to_id["awesome"]],

                       [token_to_id["statquest"], # input #2: statquest is what <EOS> awesome
                        token_to_id["is"],
                        token_to_id["what"],
                        token_to_id["<EOS>"],
                        token_to_id["awesome"]]])

## NOTE: Because we are using a Decoder-Only Transformer the outputs, or
##       the predictions, are the input questions (minus the first word) followed by
##       <EOS> awesome <EOS>.  The first <EOS> means we're done processing the input question
##       and the second <EOS> means we are done generating the output.
##       See the illustration above for more details.
labels = torch.tensor([[token_to_id["is"],
                        token_to_id["statquest"],
                        token_to_id["<EOS>"],
                        token_to_id["awesome"],
                        token_to_id["<EOS>"]],

                       [token_to_id["is"],
                        token_to_id["what"],
                        token_to_id["<EOS>"],
                        token_to_id["awesome"],
                        token_to_id["<EOS>"]]])

## Now let's package everything up into a DataLoader...
dataset = TensorDataset(inputs, labels)
dataloader = DataLoader(dataset)

# Word embedding and Position Encoding
class PositionEncoding(nn.Module):
  def __init__(self, d_model=2, max_len=6): #d_model is the number of word embeddings per token, and max_len is max number of tokens our Transformer can process
    super().__init__()
    #PE(pos, 2i) = sin(pos/100000^(2i/d(model))
    #PE(pos, 2i+1) = cos(pos/100000^(2i/d(model))
    pe = torch.zeros(max_len, d_model) #positional encoding full of zeros
    position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1) #torch arrange creates a sequence of numbers between 0 and max_len
    #floats ensures the numbers are floats, unsqueeze(1) turns sequence of numbers into a column matrix
    embedding_index = torch.arange(start=0, end=d_model, step=2).float() #represents the index i, times 2, for each word embedding, create sequence of number
    #that are between 0 and d_model
    #when d_model=2, then embedding_index is just a single value , 0: tensor([0.]), when d_model=6 its 3 values tensor([0., 2., 4.])
    div_term = 1/torch.tensor(10000.0)**(embedding_index / d_model)

    #doing the math for dividing
    pe[:, 0::2] = torch.sin(position * div_term) #assign sin function starting at column 0, and then every other column after that
    pe[:, 1::2] = torch.cos(position * div_term) #assign cos function starting at column 1, and then every other column after that

    self.register_buffer('pe', pe) #register buffer ensures that pe gets moved to a GPU if we use one

  def forward(self, word_embeddings):
    return word_embeddings + self.pe[:word_embeddings.size(0), :] #takes in word embedding values and adds the position encoding values to them

#masked self-attention
#first need to calculate query, key, and value for each token, so code up all this math
#pytorch, need to use matrix notation, basically multiply the positional encoding values for each word with the weights associated for query, key, and value
#creating an Attention class

class Attention(nn.Module):
  def __init__(self, d_model=2): #d_model is once again the dimension of the model and the number of word embedding values per token
    super().__init__()

    self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False) #creates the weight matrix that we will use to calculate the Query values Q
    #we use nn.Linear() which will create the Weight matrix and do the math for us, in_features is how many rows are in the Weight matrix, and out_features is the number of columns in the weight matrix
    # in original Transformers manuscript, theres no additional bias terms, so we dont either by setting bias=False
    #W_q is the currently untrained weights needed to calculate Query values and is Linear() object
    self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False) #same thing for keys and values
    self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)
    self.row_dim = 0 #flexibility for input data as sequentially or in batches
    self.col_dim = 1 #create some variables to keep in track of which indices oare for rows and columns

  def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None): #where we calculate the masked self attention values for each token
    #for the parameters, we are allowing the query, key, and values to be calculated from different token encodings, and flexibility to do Encoder Decoder attention if we want to
    #masked self attention prevents the model from "looking into the future", when processing sequential data and ensures the model is outputting
    #sequentially one step at a time without going ahead

    q = self.W_q(encodings_for_q) #calculating the query values for each token by passing the encodings to each Linear() object
    k = self.W_k(encodings_for_k) #calculating the query values for each token by passing the encodings to each Linear() object
    v = self.W_v(encodings_for_v) #calculating the query values for each token by passing the encodings to each Linear() object

    #ready to calculate attention
    sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim)) #multiply q by the transpose of k, calculates similarities between Queries and Keys, which is saved in sims
    scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5) #scale the similarities by the square root of the number of values used in each key
    #the scaling equation has been the standard practice since original transformer manuscript in 2017

    if mask is not None: #next is we add the mask, if we're using one, to the scaled similarities, prevents early tokens from cheating and looking at later tokens
      scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)
      '''can imagine a mask is a matrix of Trues and Falses
      and the Trues correspond to Attention values that we want to ignore, masked_fill() method replaces Trues with -1e9 (an approximation of infinity), and replaces
      the Falses with 0) '''

    attention_percents = F.softmax(scaled_sims, dim=self.col_dim)
    '''next we run the scaled similarities through a softmax() function
    applying softmax determines the percentages of influence that each token should have on each other
    '''
    attention_scores = torch.matmul(attention_percents, v) #Lastly, we use torch.matmul to multiply the attention percentages by the Values in V, which gives us the final attention scores

    return attention_scores

'''Now we create a class that puts the first three steps together, and then adds the Residual Connections,
THEN runs those values through a Fully Connected Layer, then runs them through a SoftMax() to get the outputs'''
#THIS ONE DOESNT WORK
#So we create a class called DecoderOnlyTransformer
class DecoderOnlyTransformer(L.LightningModule): #this inherents the lightnng module and take advantage of everything Lightning offers w/o overhead of inheriting it multiple times
  def __init__(self, num_tokens=4, d_model=2, max_len=6): #max_len is the maximum length of input plus output
    super().__init__()
    L.seed_everything(seed=42)

    self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model) #we create an embedding object, needs to know how many tokens are in the vocab and #values we want to represent each token
    self.pe = PositionEncoding(d_model=d_model, max_len=max_len) #then create a position encoding object
    self.self_attention = Attention(d_model=d_model) #then create an attention object
    self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens) #fully connected layer with nn.linear(), which needs to know how many inputs and outputs there are

    self.loss = nn.CrossEntropyLoss() #then a loss function to quantify how well the model performs, here we use crossentropyloss() cuz our model has multiple outputs
    #also crossentropyloss applies the softmax function within for us

  def forward(self, token_ids): #takes an array of token id numbers that will be used as inputs to the transformer
    word_embeddings = self.we(token_ids) #convert tokens into word embedding values
    position_encoded = self.pe(word_embeddings) #then we add the position encoding

    mask = torch.tril(torch.ones((token_ids.size(dim=0), token_ids.size(dim=0)), device=self.device)) #then create the mask that will prevent early tokens from looking
      #at late tokens when we calculate Attention. Start by creating a matrix of 1s with torch.ones(). Then its passed to torch.tril, leaves the
      #values in the Lower Triangle as they are and turns everything else into 0s
    mask = mask == 0 #converts 0s into Trues and 1s into Falses

    self_attention_values = self.self_attention(position_encoded, position_encoded, position_encoded, mask=mask)
      #the query, key, and values matrices will all be calculated from the same token encodings, so position_encoded is placed for all three

    residual_connection = position_encoded + self_attention_values #then we add the residual connections
    fc_layer_output = self.fc_layer(residual_connection) #lastly, everything through a Fully Connected Layer

    return fc_layer_output

    #need to write the code to train it
  def configure_optimizers(self): #a method to configure the optimizer we're using
    return Adam(self.parameters(), lr=0.1) #we're using Adam, which is the Stochastic Gradient Descent, a little less stochastic
      #also passing in all the weights and biases in the model that we want to train, which is all of them to Adam
      #setting the learning rate to 0.1 as it makes training this specific model very fast, but 0.001 is probably more commonly used

  def training_step(self, batch, batch_idx): #takes a batch of training data and an index for that batch
    input_tokens, labels = batch #split training data into inputs and labels
    output = self.forward(input_tokens[0]) #pass the input tokens into forward() that we just wrote to compute the output
    loss = self.loss(output, labels[0]) #compare the output from the Transformer to the known labels using the loss function

    return loss

#lets run the model before training it

model = DecoderOnlyTransformer(num_tokens=len(token_to_id), d_model=2, max_len=6)

#input prompt
model_input = torch.tensor([token_to_id["what"],
                            token_to_id["is"],
                            token_to_id["statquest"],
                            token_to_id["<EOS>"]])
input_length = model_input.size(dim=0) #figure out how many tokens we are using as input, cuz our super simple model can only handle a total of 6 tokens
#keeping track of how many tokens are in the input
predictions = model(model_input) #then run that through the Transformer, which generates predictions for each token in the input
predicted_id = torch.tensor([torch.argmax(predictions[-1,:])]) #we use -1 to index the outputs generated by the <EOS> token
#outputs generated by <EOS> token are an array of output values , one per possible output token
#use the argsmax token to find the output token with the largest value, which will be the first token generated as a response to the input
predicted_ids = predicted_id #save that token so we can print it out later

max_length = 6
for i in range(input_length, max_length): #use a loop to keep generating output tokens until we reach the max number of tokens that our model can generate
  if (predicted_id == token_to_id["<EOS>"]): #or the model generates the <EOS> token
    break
  model_input = torch.cat((model_input, predicted_id)) #each time we generate a new output token, we add it to the input, so each prediction is made with the full context

  predictions = model(model_input) # then the model predicts the next output token using the full context, which is input plus output tokens so far
  predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])
  predicted_ids = torch.cat((predicted_ids, predicted_id))

print("Predicted Tokens:\n")
for id in predicted_ids:
  print("\t", id_to_token[id.item()]) #print out generated tokens after converting them into id numbers to text

#Train the Decoder Only Transformer
trainer = L.Trainer(max_epochs=30)
trainer.fit(model, train_dataloaders=dataloader)

model_input = torch.tensor([token_to_id["what"],
                            token_to_id["is"],
                            token_to_id["statquest"],
                            token_to_id["<EOS>"]])
input_length = model_input.size(dim=0)

predictions = model(model_input)
predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])
predicted_ids = predicted_id

for i in range(input_length, max_length):
    if (predicted_id == token_to_id["<EOS>"]): # if the prediction is <EOS>, then we are done
        break

    model_input = torch.cat((model_input, predicted_id))

    predictions = model(model_input)
    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])
    predicted_ids = torch.cat((predicted_ids, predicted_id))

print("Predicted Tokens:\n")
for id in predicted_ids:
    print("\t", id_to_token[id.item()])